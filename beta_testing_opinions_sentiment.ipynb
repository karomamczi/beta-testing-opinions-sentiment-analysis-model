{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta Testing Opinions | Sentiment Analysis Model\n",
    "\n",
    "_Author: Karolina Mamczarz_\n",
    "\n",
    "_Based on: [Deep Learning Nanodegree Program | Udacity](https://www.udacity.com/course/deep-learning-nanodegree--nd101)_\n",
    "\n",
    "## Description\n",
    "\n",
    "PyTorch is used as a training tool. It is an open source machine learning framweork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Reaserch will use [Amazon Review Data (2018)](https://nijianmo.github.io/amazon/index.html) datasets (downloaded on March 4th, 2020):\n",
    "* [Video Games subset](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz)\n",
    "* [Software subset](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Software_5.json.gz)\n",
    "\n",
    "See citiation below:\n",
    "\n",
    "> Jianmo Ni, Jiacheng Li, Julian McAuley, **Justifying recommendations using distantly-labeled reviews and fined-grained aspects**, _Empirical Methods in Natural Language Processing (EMNLP)_, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def parse_dataset(path):\n",
    "  g = gzip.open(path, 'r')\n",
    "  for l in g:\n",
    "    yield json.loads(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_data(path):\n",
    "    data = {'pos': [], 'neg': []}\n",
    "    labels = {'pos': [], 'neg': []}\n",
    "\n",
    "    for review in parse_dataset(path):\n",
    "        if 'reviewText' in review:\n",
    "            if review['overall'] >= 4.0:\n",
    "                data['pos'].append(review['reviewText'])\n",
    "                labels['pos'].append(1)\n",
    "            elif review['overall'] <= 2.0:\n",
    "                data['neg'].append(review['reviewText'])\n",
    "                labels['neg'].append(0)\n",
    "    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        assert len(data[sentiment]) == len(labels[sentiment]), \\\n",
    "                    \"{} data size does not match labels size\".format(sentiment)\n",
    "    \n",
    "    return data, labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews Video Games: 393267 pos / 55012 neg\n",
      "Reviews Software: 8987 pos / 2219 neg\n"
     ]
    }
   ],
   "source": [
    "video_games_data, video_games_labels = get_sentiment_data('./data/Video_Games_5.json.gz')\n",
    "software_data, software_labels = get_sentiment_data('./data/Software_5.json.gz')\n",
    "\n",
    "print('Reviews Video Games: {} pos / {} neg'.format(len(video_games_data['pos']), len(video_games_data['neg'])))\n",
    "print('Reviews Software: {} pos / {} neg'.format(len(software_data['pos']), len(software_data['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sentiment_data(data1, data2, labels1, labels2):\n",
    "    data = {'pos': [], 'neg': []}\n",
    "    labels = {'pos': [], 'neg': []}\n",
    "    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        data[sentiment] = data1[sentiment] + data2[sentiment]\n",
    "        labels[sentiment] = labels1[sentiment] + labels2[sentiment]\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 402254 pos / 57231 neg\n"
     ]
    }
   ],
   "source": [
    "pre_data, pre_labels = join_sentiment_data(video_games_data, software_data, video_games_labels, software_labels)\n",
    "\n",
    "print('Data: {} pos / {} neg'.format(len(pre_data['pos']), len(pre_data['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_sentiment_data(data, limit=25000):\n",
    "    new_data = {'pos': [], 'neg': []}\n",
    "\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        new_data[sentiment] = data[sentiment][0:limit]\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 25000 pos / 25000 neg\n",
      "Labels: 25000 pos / 25000 neg\n"
     ]
    }
   ],
   "source": [
    "data = crop_sentiment_data(pre_data)\n",
    "labels = crop_sentiment_data(pre_labels)\n",
    "\n",
    "print('Data: {} pos / {} neg'.format(len(data['pos']), len(data['neg'])))\n",
    "print('Labels: {} pos / {} neg'.format(len(labels['pos']), len(labels['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentiment_data(data, labels):\n",
    "    all_data = data['pos'] + data['neg']\n",
    "    all_labels = labels['pos'] + labels['neg']\n",
    "    \n",
    "    return all_data, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 50000\n",
      "Labels: 50000\n"
     ]
    }
   ],
   "source": [
    "all_data, all_labels = combine_sentiment_data(data, labels)\n",
    "print('Data: {}'.format(len(all_data)))\n",
    "print('Labels: {}'.format(len(all_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rememb', 'first', 'time', 'laid', 'eye', 'game', 'hear', 'awesom', 'nintendo', '64', 'go', 'schoolmat', 'eagerli', 'anticip', 'usa', 'releas', 'system', 'rememb', 'hear', 'demand', 'ensu', 'shortag', 'upcom', 'christma', 'even', 'saw', 'one', 'day', 'christma', 'shop', 'dad', 'happen', 'come', 'across', 'demo', 'set', 'mall', 'final', 'got', 'see', 'massiv', 'hype', 'game', 'bright', 'color', 'mario', 'fulli', '3d', 'larg', 'crowd', 'gather', 'around', 'catch', 'glimps', 'latest', 'video', 'game', 'technolog', 'linger', 'moment', 'yet', 'first', 'impress', 'still', 'fresh', 'memori', 'mario', '64', 'look', 'like', 'noth', 'ever', 'seen', 'also', 'look', 'leap', 'bound', 'better', 'anyth', 'ever', 'seen', 'playstat', 'sega', 'saturn', 'system', 'impress', 'sever', 'month', 'later', 'final', 'save', 'enough', 'money', 'purchas', 'system', 'bundl', 'mario', '64', 'love', 'game', 'dearli', 'back', 'day', 'still', 'love', 'game', 'still', 'look', 'feel', 'play', 'unlik', 'game', 'came', 'era', 'zelda', 'ocarina', 'time', 'similar', 'graphic', 'wise', 'game', 'great', 'right', 'game', 'deserv', 'recogn', 'separ', 'mario', '64', 'perfect', 'launch', 'game', 'nintendo', '64', 'home', 'consol', 'system', 'look', 'control', 'n64', 'one', 'might', 'understand', 'skeptic', 'due', 'layout', 'howev', 'mario', '64', 'actual', 'easi', 'control', 'good', 'introduct', 'get', 'gamer', 'accustom', 'new', 'feel', 'control', 'new', 'button', 'layout', 'say', 'control', 'game', 'simpl', 'must', 'also', 'mention', 'mario', 'capabl', 'mani', 'action', 'mario', 'walk', 'run', 'crawl', 'jump', 'tripl', 'jump', 'backflip', 'side', 'somersault', 'wall', 'kick', 'swim', 'climb', 'punch', 'kick', 'slide', 'fli', 'shoot', 'cannon', 'butt', 'splash', 'hang', 'hand', 'hand', 'crouch', 'surf', 'koopa', 'shell', 'even', 'look', 'around', 'scope', 'action', 'seem', 'like', 'lot', 'like', 'difficult', 'master', 'realiti', 'ye', 'alot', 'difficult', 'master', 'skill', 'wall', 'kick', 'fli', 'take', 'practic', 'master', 'major', 'mario', 'action', 'easi', 'master', 'result', 'fluid', 'gameplay', 'sensat', 'actual', 'mario', 'camera', 'angl', 'manag', 'consid', 'control', 'might', 'find', 'slightli', 'frustrat', 'certain', 'enclos', 'area', 'boo', 'haunt', 'hous', 'part', 'camera', 'angl', 'function', 'gameplay', 'extrem', 'fun', 'gradual', 'learn', 'curv', 'need', '60', 'star', 'complet', 'game', 'player', 'look', 'one', 'challeng', 'elect', 'tri', 'collect', '120', 'star', 'requir', 'true', 'masteri', 'game', 'music', 'outstand', 'koji', 'kondo', 'also', 'famou', 'ocarina', 'time', 'music', 'music', 'mario', '64', 'succeed', 'compos', 'memor', 'enjoy', 'tune', 'person', 'favorit', 'alway', 'jolli', 'rodger', 'bay', 'dire', 'dire', 'dock', 'music', 'level', 'design', 'uniqu', 'truli', 'larg', 'level', 'offer', 'ton', 'explor', 'mario', 'find', 'sunken', 'ship', 'cave', 'toxic', 'ga', 'insid', 'volcano', 'pyramid', 'desert', 'fortress', 'sky', 'slide', 'slipperi', 'snowi', 'slope', 'even', 'insid', 'giant', 'strang', 'clock', 'game', 'fill', 'fun', 'challeng', 'hour', 'terrif', 'gameplay', 'believ', 'ever', 'met', 'anyon', 'find', 'game', 'fun', 'play', 'luigi', 'game', 'major', 'hoax', 'cite', 'ridicul', 'method', 'would', 'unlock', 'luigi', 'prove', 'untru', 'kind', 'strang', 'luigi', 'game', 'big', 'deal', 'mario', '64', 'well', 'love', 'rememb', 'kind', 'strang', 'nintendo', '64', 'launch', 'game', 'almost', 'incompar', 'entir', 'librari', 'game', 'follow', 'super', 'mario', 'world', 'super', 'nintendo', 'gem', 'great', 'game', 'sne', 'librari', 'howev', 'nintendo', '64', 'super', 'mario', '64', 'along', 'zelda', 'ocarina', 'time', 'stand', 'head', 'shoulder', 'pretti', 'much', 'everi', 'game', 'nintendo', '64', 'librari', 'disappoint', 'consid', 'see', 'nintendo', '64', 'capabl', 'support', 'top', 'notch', 'game', 'nintendo', '64', 'librari', 'manag', 'produc', 'game', 'live', 'origin', 'launch', 'game', 'end', 'mario', '64', 'alway', 'rememb', 'revolutionari', 'classic', 'liter', 'defin', 'nintendo', '64', 'bit', 'era']\n"
     ]
    }
   ],
   "source": [
    "print(review_to_words(all_data[1266]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "cache_dir = os.path.join(\"./cache\", \"sentiment_analysis\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "def reviews_to_words(data, cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if cache_data is None:\n",
    "        words = [review_to_words(review) for review in data]\n",
    "        \n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words=words)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        words = cache_data['words']\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "reviews_words = reviews_to_words(all_data)\n",
    "all_words = [item for sublist in reviews_words for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenize_words(all_words, reviews_words):\n",
    "    counts = Counter(all_words)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "    reviews_ints = []\n",
    "    for review_words in words:\n",
    "        reviews_ints.append([vocab_to_int[word] for word in review_words])\n",
    "        \n",
    "    return vocab_to_int, reviews_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 50770\n",
      "Tokenized review: \n",
      " [[1, 137, 83, 4, 1219, 11]]\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, pre_reviews_ints = tokenize_words(all_words, reviews_words)\n",
    "\n",
    "print('Unique words: {}'.format(len((vocab_to_int))))\n",
    "print('Tokenized review: \\n {}'.format(pre_reviews_ints[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove zero-length reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 55\n",
      "Maximum review length: 2996\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in pre_reviews_ints])\n",
    "\n",
    "print('Zero-length reviews: {}'.format(review_lens[0]))\n",
    "print('Maximum review length: {}'.format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_zero_length_reviews(review_ints, labels):\n",
    "    non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "    new_reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "    labels = np.array([labels[ii] for ii in non_zero_idx])\n",
    "    \n",
    "    return new_reviews_ints, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews after removing zero-length review: 49945\n",
      "Number of labels after removing zero-length review: 49945\n"
     ]
    }
   ],
   "source": [
    "reviews_ints, encoded_labels = remove_zero_length_reviews(pre_reviews_ints, all_labels)\n",
    "\n",
    "print('Number of reviews after removing zero-length review: {}'.format(len(reviews_ints)))\n",
    "print('Number of labels after removing zero-length review: {}'.format(len(encoded_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length=200):\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "        \n",
    "    assert len(features) == len(reviews_ints), \"Features should have as many rows as reviews.\"\n",
    "    assert len(features[0]) == seq_length, \"Each feature row should contain seq_length values.\"\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [  269   488   339  3626   184    17    42   339   110    35]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   13   100  2169    95   144     1    80    57  9796 10184]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [  490     1   689   122   334   144     1   112  1400  1202]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "features = pad_features(reviews_ints)\n",
    "print(features[:40,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
